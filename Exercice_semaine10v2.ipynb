{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice base sur un kernel de https://www.kaggle.com/shivamb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a> Semaine 10 les reseaux de neurone </a>\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://www.pangeanic.com/wp-content/uploads/sites/2/2017/07/neural-network-graph-624x492.jpg)\n",
    "\n",
    "I would like to thank Andrew NG and deeplearning.ai course for their excellent material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "\n",
    "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "\n",
    "Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this:\n",
    "\n",
    "000 001 002 003 ... 026 027\n",
    "028 029 030 031 ... 054 055\n",
    "056 057 058 059 ... 082 083\n",
    " |   |   |   |  ...  |   |\n",
    "728 729 730 731 ... 754 755\n",
    "756 757 758 759 ... 782 783 \n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n",
    "\n",
    "## Neural network Cheatsheet \n",
    "Qui a le temps de re-regarder les videos a la recherche de formules ;)\n",
    "\n",
    "#### C. Forward Propagation \n",
    "\n",
    "Neural Network model goes through the process called forward propagation in which it passes the computed activation outputs in the forward direction. \n",
    "\n",
    "Z = W*X + b   \n",
    "A = g(Z) \n",
    "\n",
    "- g is the activation function \n",
    "- A is the activation using the input \n",
    "- W is the weight associated with the input \n",
    "- B is the bias associated with the node \n",
    "\n",
    "#### D. Error Computation: \n",
    "\n",
    "The neural network learns by improving the values of weights and bias. The model computes the error in the predicted output in the final layer which is then used to make small adjustments the weights and bias. The adjustments are made such that the total error is minimized. Loss function measures the error in the final layer and cost function measures the total error of the network. \n",
    "\n",
    "Loss = Actual_Value - Predicted_Value   \n",
    "\n",
    "Cost = Summation (Loss)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## <a> 2. Implement a Neural Network - Binary Classification</a>  \n",
    "\n",
    "Lets implement a basic neural network in python for binary classification which is used to classify if a given image is 0 or 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "abf6aa77-17cd-4b4a-bacf-903838fe813e",
    "_uuid": "735f9f4ab4893a28cc6a28f81a49a182fd02747d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e249a1aa-a3df-4342-97ea-48a380d642a0",
    "_uuid": "3fa615d978d15f0430a3adf0bbe2b7ccc8205f5f"
   },
   "source": [
    "### 2.1 Dataset Preparation\n",
    "\n",
    "First step is to load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "34aa89c5-b5b7-4b34-9f16-0af4a4046cdf",
    "_uuid": "670c2f126fa04d98426502d720a6288e5b0cb5df"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"src/train.csv\")\n",
    "test = pd.read_csv(\"src/test.csv\")\n",
    "\n",
    "# include only the rows having label = 0 or 1 (binary classification)\n",
    "X = train[train['label'].isin([0, 1])]\n",
    "\n",
    "# target variable\n",
    "Y = train[train['label'].isin([0, 1])]['label']\n",
    "Y = Y[:,np.newaxis]\n",
    "Y = Y.T\n",
    "\n",
    "# remove the label from X\n",
    "X = X.drop(['label'], axis = 1)\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats\n",
    "\n",
    "Les resultats devraient etre pour X et Y devraient etre deux matrices de dimensions respectives\n",
    "\n",
    "X (784, 8816)\n",
    "Y (1, 8816)\n",
    "\n",
    "784 etant le nombre d'input features (des pixels allant de 0 a 255 en intensité lumineuse) et 8816 le nombre de training examples. <br>\n",
    "On retrouve ici les matrices en colones plutot qu'en ligne <br>\n",
    "Si on print Y on obtient une serie de 0 et de 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (784, 8816)\n",
      "Y (1, 8816)\n",
      "[[1 0 1 ... 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"X\", X.shape)\n",
    "print(\"Y\", Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "378fd937-d372-4eab-bcfd-46bf5817ed01",
    "_uuid": "37ea4b7a6bbedd53b00dec3cfefad523b44d68fb"
   },
   "source": [
    "### 2.2 Implementing a Activation Function \n",
    "\n",
    "We will use sigmoid activation function because it outputs the values between 0 and 1 so its a good choice for a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "19fc3ff1-f6ac-447d-98ea-4ff4d4bf4669",
    "_uuid": "13f0338a967a4cb87503821f044c09552d2e1640"
   },
   "outputs": [],
   "source": [
    "# implementing a sigmoid activation function\n",
    "def sigmoid(x, derivative=False):\n",
    "    if(derivative == True):\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats\n",
    "\n",
    "On peut verifier son resultat avec les valeurs connues de la fontion sigmoid comme : <br>\n",
    "sigmoid(0) = 0.5 <br>\n",
    "sigmoid(0, derivative = False) = 0.25 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0))\n",
    "print(sigmoid(0, derivative = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee1e1ebe-4298-40cd-9044-d8dc4327aa6c",
    "_uuid": "3c58c24a7feb9789a5bd84e8c9cc5068eebbee04"
   },
   "source": [
    "### 2.3 Define Neural Network Architecture\n",
    "\n",
    "Create a model with three layers - Input, Hidden, Output.\n",
    "\n",
    "Nous voulons creer ici une architecture pour notre reseau de neurone : combien de layers ? d'units pour chaque layer ect ... <br>\n",
    "<br>\n",
    "Nous commencerons par un reseau a 2 layers : 1 input layer, 1 hidden layer et 1 output layer : <br>\n",
    " - Un input de 784 (le nombre de feature pour chaque image) <br>\n",
    " - 10 unité dans le hidden layer <br>\n",
    " - Seulement une uniteé dans l'output layer qui vaudra zero si notre image represente un 0 et ... 1 si c'est un 1 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "a4c98dcc-6a34-4e5d-99d7-7e26ec12ce21",
    "_uuid": "deab1ad6439f304b669f70da3ca1f70e13f027f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x 784\n",
      "n_h 10\n",
      "n_y 1\n"
     ]
    }
   ],
   "source": [
    "def network_architecture(X, Y):\n",
    "    # nodes in input layer\n",
    "    n_x = X.shape[0]\n",
    "    # nodes in hidden layer\n",
    "    n_h = 10         \n",
    "    # nodes in output layer\n",
    "    n_y = 1\n",
    "    return (n_x, n_h, n_y)\n",
    "\n",
    "n_x, n_h, n_y = network_architecture(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc75a514-66e5-4479-9613-7fdc4572271d",
    "_uuid": "c91f07b7c28808b74babea3d013fbfdd6ea3ba91"
   },
   "source": [
    "### 2.4 Define Neural Network Parameters \n",
    "\n",
    "Neural Network parameters are weights and bias which we need to initialze with zero values. The first layer only contains inputs so there are no weights and bias, but the hidden layer and the output layer have a weight and bias term. (W1, b1 and W2, b2) <br> <br>\n",
    "\n",
    "Afin d'initializer aleatoirement nos poids on va utiliser une fonction de numpy qui return une matrice de dimension de notr choix avec des issues d'une distribution normale que l'on mettra a l'echelle 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "cef24778-cd2c-4412-b017-7f4d9b34f34a",
    "_uuid": "c13e2d5cd4d04c05c878f7ccab1677b290847be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (10, 784)\n",
      "b1 (10, 1)\n",
      "w2 (1, 10)\n",
      "b2 (1, 1)\n"
     ]
    }
   ],
   "source": [
    "def define_network_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01 # random initialization\n",
    "    b1 = np.zeros((n_h, 1)) # zero initialization\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01 \n",
    "    b2 = np.zeros((n_y, 1)) \n",
    "\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "params = define_network_parameters(n_x, n_h, n_y)\n",
    "print(\"w1\", params['W1'].shape)\n",
    "print(\"b1\", params['b1'].shape)\n",
    "print(\"w2\", params['W2'].shape)\n",
    "print(\"b2\", params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On devrait avoir les dimensions suivantes : \n",
    "w1 (10, 784)\n",
    "b1 (10, 1)\n",
    "w2 (1, 10)\n",
    "b2 (1, 1)\n",
    "\n",
    "\n",
    "Et puis ca ne fait jamais de mal de visualiser de temps en temps ;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = network_architecture(X, Y)\n",
    "\n",
    "params = define_network_parameters(n_x, n_h, n_y)\n",
    "\n",
    "print(n_x, n_h, n_y)\n",
    "print(params['W1'])\n",
    "print(params['W2'])\n",
    "print(params['b1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd1fd5e9-aae9-4996-8137-6f577147b90c",
    "_uuid": "9664d29beedd6c5a7c27b10201992a3c2f810c56"
   },
   "source": [
    "### 2.5 Implement Forward Propagation\n",
    "\n",
    "The hidden layer and output layer will compute the activations using sigmoid activation function and will pass it in the forward direction. While computing this activation, the input is multiplied with weight and added with bias before passing it to the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "2fd5cfe1-dbe2-4580-b541-1ed5cf139166",
    "_uuid": "ee7d22c45eeb9768b2722c36ecf9f73ff9ef2db9"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    Z1 = np.dot(params['W1'], X) + params['b1']\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    Z2 = np.dot(params['W2'], A1) + params['b2']\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats\n",
    "\n",
    "On peut au moins verifier la forme des matrices calculées : <br>\n",
    "Z1  (10, 8816)<br>\n",
    "A1  (10, 8816)<br>\n",
    "Z2  (1, 8816) <br>\n",
    "A2  (1, 8816) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (10, 8816)\n",
      "A1 (10, 8816)\n",
      "Z2 (1, 8816)\n",
      "A2 (1, 8816)\n"
     ]
    }
   ],
   "source": [
    "refor = forward_propagation(X, params)\n",
    "\n",
    "print(\"Z1\", refor['Z1'].shape)\n",
    "print(\"A1\", refor['A1'].shape)\n",
    "print(\"Z2\", refor['Z2'].shape)\n",
    "print(\"A2\", refor['A2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da5fac38-7256-4fc3-bcfc-b2358b8bff31",
    "_uuid": "162413bf075111e6355ee9d5f16d0ca78a2f33a9"
   },
   "source": [
    "### 2.6 Compute the Network Error \n",
    "\n",
    "To compute the cost, one straight forward approach is to compute the absolute error among prediction and actual value. But a better loss function is the log loss function which is defines as : \n",
    "\n",
    "  -Summ ( Log (Pred) * Actual + Log (1 - Pred ) * Actual ) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "155f019d-3e33-4cdd-9722-4d0a7121276f",
    "_uuid": "08d7b796a3dbdcb1530053238037803ac76362c3"
   },
   "outputs": [],
   "source": [
    "def compute_error(Predicted, Actual):\n",
    "    logprobs = np.multiply(np.log(Predicted), Actual)+ np.multiply(np.log(1-Predicted), 1-Actual)\n",
    "    cost = -np.sum(logprobs) / Actual.shape[1] \n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c4ef52b-55ac-4ef2-a5fd-ee164db8dcdc",
    "_uuid": "58d61c5257c8f568d439a605fc311f483f09f1e2"
   },
   "source": [
    "### 2.7 Implement Backward Propagation\n",
    "\n",
    "In backward propagation function, the error is passed backward to previous layers and the derivatives of weights and bias are computed. The weights and bias are then updated using the derivatives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "8796bbb2-8fbf-4740-9687-c3ec2c38a27f",
    "_uuid": "195d2b62d4655b7ecff22e4bc70be15256aadf1b"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(params, activations, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # output layer\n",
    "    dZ2 = activations['A2'] - Y # compute the error derivative \n",
    "    dW2 = np.dot(dZ2, activations['A1'].T) / m # compute the weight derivative \n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m # compute the bias derivative\n",
    "    \n",
    "    \n",
    "    # hidden layer\n",
    "    dZ1 = np.dot(params['W2'].T, dZ2) * sigmoid(activations['Z1'], derivative=True)\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1,keepdims=True) / m\n",
    "    \n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def update_parameters(params, derivatives, alpha = 1.2):\n",
    "    # alpha is the model's learning rate \n",
    "    \n",
    "    params['W1'] = params['W1'] - alpha * derivatives['dW1']\n",
    "    params['b1'] = params['b1'] - alpha * derivatives['db1']\n",
    "    params['W2'] = params['W2'] - alpha * derivatives['dW2']\n",
    "    params['b2'] = params['b2'] - alpha * derivatives['db2']\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois la meilleure verification consiste a verifier ses dimensions et a jeter un oeuil a son resultat :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = update_parameters(params, backward_propagation(params, results, X, Y))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "935845e8-c35f-43ae-8f28-65eee43fc428",
    "_uuid": "1724d52016b387a2f974e6c6fc27c2a17a96166b"
   },
   "source": [
    "### 2.8 Compile and Train the Model\n",
    "\n",
    "Create a function which compiles all the key functions and creates a neural network model.\n",
    "\n",
    "Maintenant que l'on a poser une a une les pierres de notre algorythme il est temps de prendre un peu de recul pour verifier que tout s'articule et fonctionne de maniere harmonieuse : <br>\n",
    " - Il faut definir l'architecture de notre reseau\n",
    " - initialiser nos parametres\n",
    " - puis pour un nombre donné d'époches on fait une prediction de resultat puis on affine nos parametres (gradient descent) ... notre algorythme \"apprend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "84c99459-755e-4795-a52f-bb8cf59299a4",
    "_uuid": "011debd87ff12ce75d29dac9383d74db9c64a35a"
   },
   "outputs": [],
   "source": [
    "def neural_network(X, Y, num_iterations = 10):\n",
    "    n_x, n_h, n_y = network_architecture(X, Y) #on definit l'architecture de notre reseau\n",
    "    \n",
    "    params = define_network_parameters(n_x, n_h, n_y) #on en profite pour initialiser aleatoirement nos parametres\n",
    "    for i in range(0, num_iterations):\n",
    "        activations = forward_propagation(X, params)\n",
    "        \n",
    "    #    error = compute_error(results['A2'], Y)\n",
    "    #    print(error)\n",
    "    \n",
    "        derivatives = backward_propagation(params, activations, X, Y) \n",
    "        params = update_parameters(params, derivatives)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se lance maintenant !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "fb2fa7af-e787-4ca1-9079-f6dfb6723390",
    "_uuid": "2d983cbb3c77270b2ee66d9148315cb24888f292"
   },
   "outputs": [],
   "source": [
    "model = neural_network(X, Y, num_iterations = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6dccfd5a-8850-41a0-b406-47460f5bd2ed",
    "_uuid": "dc33c60645cdaba20c5a420ab6bc7e5933b6e88b"
   },
   "source": [
    "### 2.9 Predictions \n",
    "\n",
    "La fonction predict va nous permettre a partir de nos parametres fraichement affinés de formuler avec une n-ième forward-propagation la fiabilité de nos predictions sur notre training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "59eed767-201d-4138-a77e-2ce8eaad83df",
    "_uuid": "df27eb0a992e1d7646c2ef5f5c7ca928bca6155e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99599353e-01 2.95189628e-04 9.99599353e-01 ... 9.99599353e-01\n",
      " 2.95189628e-04 9.99599353e-01]\n"
     ]
    }
   ],
   "source": [
    "def predict(parameters, X):\n",
    "    results = forward_propagation(X, parameters)\n",
    "    print (results['A2'][0])\n",
    "    predictions = np.around(results['A2'])    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict(model, X)\n",
    "# print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "i =  332\n",
      "i =  1364\n",
      "i =  1370\n",
      "i =  5136\n",
      "i =  5818\n",
      "i =  7653\n",
      "[332, 1364, 1370, 5136, 5818, 7653]\n",
      "Accuracy: 99.234694%\n"
     ]
    }
   ],
   "source": [
    "#Error va contenir l'index (sur nos 8860 training examples) des exemples ou notre algorythme s'est trompé\n",
    "\n",
    "Error = []\n",
    "for i in range(Y.shape[1]):\n",
    "    if (round(Y[0][i], 0) != predictions[0][i]):\n",
    "        print(\"i = \", i)\n",
    "        Error.append(i)\n",
    "print(Error)\n",
    "        \n",
    "print ('Accuracy: %f' % float((float(X.shape[0]) - float(len(Error))) / float(X.shape[0]) * 100) + '%')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib as plt\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "columns = 4\n",
    "rows = 5\n",
    "for i in range(1, len(Error)):\n",
    "    \n",
    "    X1 = X.T.iloc[Error[i]]\n",
    "    X1 = X1.values.reshape(28,28)\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(X1, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
