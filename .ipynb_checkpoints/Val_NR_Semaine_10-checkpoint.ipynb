{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 11 - Réseau de neurones de base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette semaine nous allons écrire un réseau de neurones de base, que nous allons entraîner afin qu'il inverse des séquences de bits. Si vous réussissez à l'implémenter, vous pourrez ensuite vous amuser à l'utiliser sur d'autres types de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations et initialisations de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous n'allons utiliser que numpy pour cet exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons un réseau à deux couches (l'input ne comptant pas pour une couche). Nous allons utiliser 300 séquences de bits pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nb de neurones sur chaque couche\n",
    "n_in = 10\n",
    "n_hidden = 8\n",
    "n_out = 10\n",
    "\n",
    "# Nb de 'training examples'\n",
    "m = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 1  # Learning rate\n",
    "epochs = 200  # nb iterations du gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des fonctions d'activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utiliserons la fonction tanh pour l'activation de la \"hidden layer\", et la sigmoïde pour la dernière couche. Implémentez-les si elle n'existent pas déjà dans numpy. Implémentez aussi la dérivée de l'une ou l'autre d'entre elles, le cas échéant.\n",
    "Attention! Les fonctions doivent pouvoir traiter des vecteurs ou des matrices en effectuant l'opération sur chaque élément de ces derniers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dérivée de tanh\n",
    "def tanh(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return (1 - (x ** 2))\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-99.     0.96   0.  ]\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([10, 0.2, -1])\n",
    "\n",
    "print(tanh(Z, derivative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons écrire une fonction qui fait une activation puis une rétropropagation, puis renvoie l'erreur (loss) et le gradient (toutes ces variables qui commencent par d...). L'itération sur les 200 epochs se fera dans un deuxième temps.\n",
    "\n",
    "\n",
    "Question, pourquoi b1 et b2 (les biaised unites), sont des matrices initalizées a 0\n",
    "Les dimensoins des matrices d'input ect sont bien inversées pour les NN car ca rend les calculs plus simples\n",
    "\n",
    "        # Backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    " alors que pour calculer delta ANg fait delta3 = W3(transposed).delta4 .* g'(Z3)\n",
    " et pas de trace des biased units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, Y, W1, W2, b1, b2):\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward\n",
    "        Z2 = np.transpose(W1).dot(X) + b1\n",
    "        A2 = tanh(Z2)\n",
    "#         A2\n",
    "        Z3 = np.transpose(W2).dot(A2) + b2\n",
    "        A3 = sigmoid(Z3)\n",
    "\n",
    "#     print(A2, A3)\n",
    "        # Backward\n",
    "        delta3 = A3 - Y\n",
    "#         print(delta3.shape)\n",
    "#         print(A2.shape)\n",
    "#         dW2 = A2.T.dot(delta3)\n",
    "        dW2 = A2.dot(delta3.T)\n",
    "        db2 = np.sum(delta3)\n",
    "        delta2 = W2.dot(delta3) * sigmoid(A2, derivative=True)\n",
    "#         print(delta2.shape)\n",
    "#         print(X.shape)\n",
    "        dW1 = X.dot(delta2.T)\n",
    "        db1 = np.sum(delta2)\n",
    "#     print(A3 - Y)\n",
    "\n",
    "        # Parameter update (use the learning rate alpha here!)\n",
    "#         print(dW2.shape)\n",
    "#         print(W2.shape)\n",
    "        \n",
    "        W1 += dW1 * -1\n",
    "        b1 += -alpha * db1\n",
    "        W2 += -alpha * dW2\n",
    "        b2 += -alpha * db2\n",
    "    \n",
    "        # Compute loss\n",
    "        loss = np.sum(delta3)\n",
    "        \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "#         print(\"Epoch %d, Loss: %.8f\" % (epoch, loss))\n",
    "    \n",
    "    return loss_history, W1, W2, b1, b2\n",
    "\n",
    "# prin(train(X, Y, W1, W2, b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation des paramètres du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, certains paramètres sont initalisés à zéro, d'autres non..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 8)\n"
     ]
    }
   ],
   "source": [
    "# W1 10 x 8 car elle permet de passer de in 10 a h 8\n",
    "# W2 8 x 10 car output = 10\n",
    "# b1 = 8\n",
    "# b2 = 10\n",
    "np.random.seed(0)\n",
    "X = np.random.binomial(1, 0.5, (n_in, m))\n",
    "Y = X ^ 1\n",
    "\n",
    "input_dim = X.shape[0]\n",
    "output_dim = Y.shape[0]\n",
    "hidden_dim = 8\n",
    "\n",
    "# print(X)\n",
    "# print(Y)\n",
    "\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "b1 = 0\n",
    "b2 = 0\n",
    "\n",
    "print(W1.shape)\n",
    "# W2 = \n",
    "# b1 = \n",
    "# b2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des données d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici il s'agit créer 300 séries de 10 chiffres binaires (1 et 0) pour les X.\n",
    "Les Y seront ces mêmes séries, inversées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 1 0 0]\n",
      " [1 0 1 ... 1 1 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [1 0 1 ... 0 0 1]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "[[1 1 1 ... 1 1 0]\n",
      " [1 0 1 ... 0 1 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [1 1 1 ... 0 1 1]\n",
      " [0 1 0 ... 1 1 0]\n",
      " [0 1 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Data generation\n",
    "X = np.random.binomial(1, 0.5, (n_in, m))\n",
    "Y = X ^ 1\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lancer l'entraînement du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1073185527.1211963\n"
     ]
    }
   ],
   "source": [
    "loss_history, W1, W2, b1, b2 = train(X, Y, W1, W2, b1, b2)\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiser la décroissance de l'erreur sur un graphe (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1225.0007175821925, 1474.999999999991, -1525.0, 1175.0, -1225.0, 1475.0, -1525.0, 1475.0, -1225.000000034107, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1474.9999999999827, -1525.0, 1360.8284088928647, -1524.9999980442435, 1475.0, -1224.9999999999998, 1475.0, -1525.0, 1475.0, -1225.0, 1174.8005258151204, -1225.0000000208515, 1175.0, -1224.8635413211882, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1524.9999009025555, 1475.0, -1225.0, 875.0, -625.0, 275.0, 275.0, -925.0, 1175.0, -1525.0, 1475.0, -1525.0, 1475.0, -1225.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1225.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1225.0, 1475.0, -1525.0, 1475.0, -1225.0, 275.0, 275.0, -925.0, 1174.9999999998274, -1225.0, 1475.0, -1525.0, 1475.0, -1225.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1225.0, 1175.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1522.5889073402416, 1475.0, -925.0, 1175.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1225.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1175.0032359805107, -625.0, -25.0, 1175.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1225.0, 1175.0, -1525.0, 1475.0, -1224.9970173120369, 1175.026802434723, -1224.9999999935578, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1232.3237844973537, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -925.2393697041258, 275.0, 574.9999999746675, -1225.0, 1475.0, -1525.0, 1475.0, -1225.0, 1175.0, -1525.0, 1475.0, -1225.005056710738, 1475.0, -1525.0, 1474.9999994246714, -1525.0, 1175.0, -1525.0, 1475.0, -1225.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0, -1225.0000000000005, 1475.0, -1525.0, 1475.0, -1225.0, 1474.9999999968613, -1525.0, 1474.9962253528984, -1525.0, 1194.1986071549109, -1225.0, 1475.0, -1525.0, 1475.0, -1525.0, 1475.0]\n"
     ]
    }
   ],
   "source": [
    "print(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation du réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Écrivez une petite fonction qui, à partir des activation de la dernière couche du réseau, produit un vecteur de 1 et de 0. Normalement il suffit de copier-coller quelque lignes de code et d'ajouter quelque chose à la fin. Attention, ici, contrairement à ce qu'on avait dans le MOOC, la dernière couche a 10 valeurs de sortie, et non pas une seule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, W1, W2, b1, b2):\n",
    "    Z2 = np.transpose(W1).dot(X) + b1\n",
    "    A2 = tanh(Z2)\n",
    "    Z3 = np.transpose(W2).dot(A2) + b2\n",
    "    A3 = sigmoid(Z3)\n",
    "    return(A3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester la performance sur un seul exemple\n",
    "Ici on génère un seul exemple (une série de 10 chiffres binaires), puis on fait prédire son inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 0 1 0]]\n",
      "[[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  1.68055973e-228 0.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.binomial(1, 0.5, (n_in,1))\n",
    "print(X.T)\n",
    "print(predict(X, W1, W2, b1, b2).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester la performance sur une série d'exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
